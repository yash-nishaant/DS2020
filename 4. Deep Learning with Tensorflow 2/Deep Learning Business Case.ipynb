{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audiobook App Business Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The given dataset contains information about customer purchases from an audiobook app. Each customer in the database has made at least one purchase from the app. The following is an implementation of a deep learning model designed with Tensorflow 2 which seeks to examine the <b>likelihood of these customers making a purchase from the app again</b>. The motivation behind answering this question is to determine the most effective methods and userbase to advertise to. It is reasonable to assume that given information about the purchasing habits and experience of customers, and targetting those that are predicted to be most likely to buy again through a well-trained model will yield the best, most targeted and accurate results regarding potential future customers and allow the app company to design the optimal ad campaign.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data provides us with several important variables to consider. We have standard attributes about the purchase such as the price and minutes listened, effective indicators of how big a priority audiobooks are in the customer's life. We also have consumer engagement metrics such as review status and score, number of support requests and the difference between the last app visit and the purchase date.  \n",
    "\n",
    "The data was gathered from an audiobook app and contains 2 years worth of customer engagement data. This data is contained in the loaded <code>.csv</code> file. There was, however, a further 6 months worth of consumer data analyzed to determine if there had been a purchase from the customer in that time period. The results are the boolean targets for the model. If there have been no purchases in the 6-month period, it is safe to assume that the customer has moved on to a different provider or just stopped buying audiobooks.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "raw_data = np.loadtxt('Audiobooks_data.csv', delimiter=',')\n",
    "\n",
    "unscaled_inputs = raw_data[:,1:-1]\n",
    "targets = raw_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling\n",
    "shuffled_indices = np.arange(unscaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "# Using shuffle indices to shuffle inputs and targets\n",
    "unscaled_inputs = unscaled_inputs[shuffled_indices]\n",
    "targets = targets[shuffled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the dataset\n",
    "num_true_targets = int(np.sum(targets)) # Count the targets that are 1's (customer bought a book in the last 6 months)\n",
    "zero_targets_counter = 0 # set a counter for number of targets that are 0's (customer didn't buy)\n",
    "indices_to_remove = [] # array containing input-target pairs which need to removed to create a balanced dataset\n",
    "\n",
    "for i in range(targets.shape[0]):\n",
    "    if targets[i] == 0:\n",
    "        zero_targets_counter += 1\n",
    "        if zero_targets_counter > num_true_targets:\n",
    "            indices_to_remove.append(i)\n",
    "\n",
    "# These two variables contain the inputs and targets respectively\n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs, indices_to_remove, axis=0)\n",
    "targets_equal_priors = np.delete(targets, indices_to_remove, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization using sklearn's preprocessing capabilities\n",
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling after scaling\n",
    "# Data collected arranged by date. shuffle indices to ensure data is not arranged in the same way as it is fed.\n",
    "# Since we are batching, data should be spread out as randomly as possible\n",
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "# Use the shuffled indices to shuffle inputs and targets\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1776.0 3579 0.49622799664710815\n",
      "246.0 447 0.5503355704697986\n",
      "215.0 448 0.4799107142857143\n"
     ]
    }
   ],
   "source": [
    "# Split the datasets into training, validation and test sets\n",
    "\n",
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "train_samples_count = int(0.8 * samples_count)\n",
    "validation_samples_count = int(0.1 * samples_count)\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]\n",
    "\n",
    "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
    "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
    "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the datasets in an .npz file\n",
    "\n",
    "np.savez('Audiobooks_data_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('Audiobooks_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
    "np.savez('Audiobooks_data_test', inputs=test_inputs, targets=test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz = np.load('Audiobooks_data_train.npz')\n",
    "train_inputs = npz['inputs'].astype(np.float)\n",
    "train_targets = npz['targets'].astype(np.int)\n",
    "\n",
    "npz = np.load('Audiobooks_data_validation.npz')\n",
    "validation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n",
    "\n",
    "npz = np.load('Audiobooks_data_test.npz')\n",
    "test_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "\n",
    "The model outline, loss function, early stopping and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "36/36 - 0s - loss: 0.5931 - accuracy: 0.6683 - val_loss: 0.5052 - val_accuracy: 0.7539\n",
      "Epoch 2/100\n",
      "36/36 - 0s - loss: 0.4781 - accuracy: 0.7575 - val_loss: 0.4466 - val_accuracy: 0.7808\n",
      "Epoch 3/100\n",
      "36/36 - 0s - loss: 0.4341 - accuracy: 0.7712 - val_loss: 0.4123 - val_accuracy: 0.7987\n",
      "Epoch 4/100\n",
      "36/36 - 0s - loss: 0.4085 - accuracy: 0.7849 - val_loss: 0.3913 - val_accuracy: 0.8166\n",
      "Epoch 5/100\n",
      "36/36 - 0s - loss: 0.3931 - accuracy: 0.7952 - val_loss: 0.3786 - val_accuracy: 0.8255\n",
      "Epoch 6/100\n",
      "36/36 - 0s - loss: 0.3827 - accuracy: 0.7969 - val_loss: 0.3729 - val_accuracy: 0.8210\n",
      "Epoch 7/100\n",
      "36/36 - 0s - loss: 0.3761 - accuracy: 0.7999 - val_loss: 0.3658 - val_accuracy: 0.8166\n",
      "Epoch 8/100\n",
      "36/36 - 0s - loss: 0.3714 - accuracy: 0.8036 - val_loss: 0.3665 - val_accuracy: 0.8166\n",
      "Epoch 9/100\n",
      "36/36 - 0s - loss: 0.3691 - accuracy: 0.8019 - val_loss: 0.3605 - val_accuracy: 0.8143\n",
      "Epoch 10/100\n",
      "36/36 - 0s - loss: 0.3649 - accuracy: 0.8011 - val_loss: 0.3556 - val_accuracy: 0.8277\n",
      "Epoch 11/100\n",
      "36/36 - 0s - loss: 0.3616 - accuracy: 0.8108 - val_loss: 0.3534 - val_accuracy: 0.8210\n",
      "Epoch 12/100\n",
      "36/36 - 0s - loss: 0.3639 - accuracy: 0.8066 - val_loss: 0.3530 - val_accuracy: 0.8210\n",
      "Epoch 13/100\n",
      "36/36 - 0s - loss: 0.3614 - accuracy: 0.8075 - val_loss: 0.3521 - val_accuracy: 0.8300\n",
      "Epoch 14/100\n",
      "36/36 - 0s - loss: 0.3561 - accuracy: 0.8103 - val_loss: 0.3518 - val_accuracy: 0.8255\n",
      "Epoch 15/100\n",
      "36/36 - 0s - loss: 0.3557 - accuracy: 0.8122 - val_loss: 0.3515 - val_accuracy: 0.8233\n",
      "Epoch 16/100\n",
      "36/36 - 0s - loss: 0.3544 - accuracy: 0.8117 - val_loss: 0.3510 - val_accuracy: 0.8255\n",
      "Epoch 17/100\n",
      "36/36 - 0s - loss: 0.3517 - accuracy: 0.8131 - val_loss: 0.3479 - val_accuracy: 0.8300\n",
      "Epoch 18/100\n",
      "36/36 - 0s - loss: 0.3515 - accuracy: 0.8086 - val_loss: 0.3476 - val_accuracy: 0.8233\n",
      "Epoch 19/100\n",
      "36/36 - 0s - loss: 0.3504 - accuracy: 0.8120 - val_loss: 0.3463 - val_accuracy: 0.8322\n",
      "Epoch 20/100\n",
      "36/36 - 0s - loss: 0.3513 - accuracy: 0.8120 - val_loss: 0.3512 - val_accuracy: 0.8233\n",
      "Epoch 21/100\n",
      "36/36 - 0s - loss: 0.3490 - accuracy: 0.8114 - val_loss: 0.3450 - val_accuracy: 0.8322\n",
      "Epoch 22/100\n",
      "36/36 - 0s - loss: 0.3482 - accuracy: 0.8117 - val_loss: 0.3475 - val_accuracy: 0.8322\n",
      "Epoch 23/100\n",
      "36/36 - 0s - loss: 0.3493 - accuracy: 0.8103 - val_loss: 0.3537 - val_accuracy: 0.8255\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24bc4b78ac8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 10\n",
    "output_size = 2\n",
    "hidden_layer_size = 50\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                            tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "                            ])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) # Adam optimizer awesome! \n",
    "# sparse_categorical_crossentropy is chosen as loss function because it is a classification problem dealing with categorical\n",
    "# data and the function also one-hot encodes our targets, allowing for greater convenience.\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "batch_size = 100\n",
    "max_epochs = 100\n",
    "\n",
    "#early stopping mechanism\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience = 2) # Early stopping mechanism. Patience of 2 implies slight \n",
    "                                                               # tolerance against random validation loss increases\n",
    "\n",
    "# Training\n",
    "model.fit(train_inputs, \n",
    "          train_targets, \n",
    "          batch_size=batch_size, \n",
    "          epochs = max_epochs, \n",
    "          callbacks=[early_stopping], # callbacks are functions called after execution of task. \n",
    "          validation_data=(validation_inputs, validation_targets), \n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Model\n",
    "\n",
    "Test the predictive power of the model by introducing it to test data it has never encountered before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 855us/step - loss: 0.3459 - accuracy: 0.8214\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test loss: 0.35. Test accuracy: 82.14%\n"
     ]
    }
   ],
   "source": [
    "print('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
